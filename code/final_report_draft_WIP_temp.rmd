---
title: 'You Get What You Pay For: Experimental Analysis on the Relationship Between Pay and Work Quality'
output:
  pdf_document:
    number_sections: true
---

```{r,include=FALSE}
rm(list = ls())
```

```{r, include=FALSE}
#setwd("D:/MIDS/W241_1_Experiments_Causality/project/Field_Experiment_Human_Image_Classification/code")
#setwd("F:/001_Learn_UCB/241_Experiments_and_Causality/Field_Experiment_Human_Image_Classification/code")
setwd("C:/Users/chuco/Documents/Repo_Field_Experiment_Human_Image_Classification/code")
source(file = "design1_data_transformation_functions.r")
source(file = "design1_data_analysis_functions.r")
source(file = "design2_data_transformation_functions.r")
source(file = "design2_data_analysis_functions.r")
```

\centerline{\Large Legg Yeung, Stanimir Vichev, Frederic Suares}
\bigskip
\centerline{\Large University of California, Berkeley}
\bigskip
\centerline{\Large December 17, 2017}
\bigskip
\bigskip
\bigskip
\bigskip
\centerline{\bfseries Abstract}

> Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean a sem vitae mi lobortis condimentum. Maecenas rutrum vitae libero in tempor. Fusce tempor mauris nec vulputate luctus. Phasellus et semper nisi, sed luctus erat. Duis eu congue lectus, ac mattis diam. Fusce tristique efficitur lacus, vel vehicula leo mollis id. Proin vel venenatis velit. Sed et semper ante. Interdum et malesuada fames ac ante ipsum primis in faucibus. Sed id mi urna. Integer urna nunc, feugiat sed felis vitae, imperdiet tristique ipsum. Sed pretium eros massa, a sollicitudin diam finibus aliquet.

\centerline{\bfseries Fix section numbers in the introduction}

> Add optional table of contents here

\pagebreak

# Introduction

In most economies, it is generally believed that remuneration for someone's work is strongly related to the effort they will put into it, and the eventual quality of the results. Unfortunately, this is a concept that is challenging to test in the normal world. Employers cannot easily conduct experiments with their own employees, say, by giving them similar tasks and different payments on a random basis, as this could be considered unethical and would lead to a serious disruption in the workforce. At the same time, such a study would be very helpful to employers who want to understand what motivates their employees, and what part the pay plays. 


The Amazon Mechanical Turk (AMT) platform for the crowdsourced completion of tasks provides a great opportunity for experimentally testing the relationship between payment and quality of work without having to worry about subject interaction or high costs of wasted man-hours. Our experiment uses the AMT platform to experimentally test whether higher payment for a task has a positive effect on the quality of its result. We used two different experimental designs (traditional between-subject and stepped-wedge), one randomly assign tasks to four different payment levels and the other randomly assign turkers to four different payment levels, to measure how resultant quality of work differ between groups.


The paper is organized as follows: section 2 discusses background and motivations, section 3 explains the experimental design, detailing the platforms used and the experimental schedule, sections 4 and 5 present the data and analysis for the two experiment designs, section 6 discusses overall results, section 7 looks at possible future studies, followed by a conclusion and bibliography.
 

# Related Work

The use of online labor markets as an effective and efficient platform for social science experimentation has been noted by several studies, and explored in detail in Horton et. al. 2011. They perform several successful experiments and even look at the labor supply curves of workers. This shows that we have made the right choice of platform to conduct our research. Another experiment done by Horton  & Chilton, 2010, develops a novel method for estimating the smallest price for a task that a worker would accept. They also look into the way workers respond to incentives, with some being rational and some setting earnings targets. Finally, Mason & Watts, 2009, use the AMT platform to explore the effect of financial incentives on the performance of workers. They conclude that higher financial incentives increase the quantity, but not quality, of the work done by workers, citing an anchoring effect as the cause of this. By doing a similar experiment nearly 9 years later, we hope to see whether we get the same results as online labor markets such as AMT gain more prominence and popularity, leading to a more diverse market with more workers and requestors. 



# Research Hypothesis, Identification Strategy

We hypothesis that higher payment per human intelligence task (HIT) on average would lead to higher task performance. To operationalize this construct, we define the treatment variable as pay rate in US dollars, and outcome variable as proportion of image classification questions scored correctly in each returned HIT. In each of the two experimental designs, four different pay rates are randomly assigned to each HIT. Similarly, in each of the two experimental designs, a total of 50 image classifications questions on dog breeds are prompted in each HIT. The four pay rates are chosen between \$0.10 and \$0.55, which correspond to the lower and upper bound we commonly see for similar image classification tasks on the AMT platform. We chose image classification, instead of other common HIT categories such as audio transcription, key point identification, or text responses because the correct answers tend to be unequivocal. To identify the treatment effect, our main approach is to regress task level performance on the assigned pay rate, controlling for other pre-treatment covariates for better precision. The resultant coefficient of assigned pay rate should be an estimate of the average pay rate effect on task level accuracy. We will walk through the motivations, designs, protocols and models for the two designs in the following sections.



# Experimental Design and Protocol

Our experiment connect the AMT platform HIT work flow with the Qualtrics platform survey work flow. The AMT platform allows us, as a requestor to post HITs of different treatment pay rates and availabilities. Once a turker select our HIT out of a list of other HITs from other requestors based on our pay rate and description(printed below), the turker will be directed to our Qualtrics survey through a web link. Once all the survey questions are completed and the Qualtrics survey ends, the turker will submit their identification number of the AMT platform again. Once all the available HITs for a particular posting are claimed, completed and submitted by turkers, both the AMT posting and Qualtrics survey are terminated. Finally, we download data from both platforms, conduct statistical analyses and reward turkers who score higher than a pre-determined accuracy threshold. 

>*Title : Multiple-Choice Task*
>*Description: This is a 50-question multiple choice task*
>*Keywords: survey, multiple-choice*

>*Reward per assignment: 0.1*
>*Time allotted: 20min* (If this is too long, turkers may think this is a very hard task)

>*We need help with this multiple-choice task, which will provide us examples to train a computation model. The survey consist of several demographic questions, followed by 50 multiple choice questions. You don't need any prior experience or knowledge to complete this task. Select the link below to complete the survey. At the end of the survey, you will receive a code to paste into the box below to receive credit for taking our survey.*

>*Make sure to leave this window open as you complete the survey. When you are finished, you will return to this page to paste the code into the box.*

>(And below this they see the survey link and the box to enter the code.)



The Qualtrics survey begins by prompting for the turker's identification number and a block of 5 forced-response, multiple-choice questions to probe the turker's aptitude for dog-breed classification. Up until this point, the turker has no knowledge that this is an image classification task, nor relevancy to dog breeds. The turker cannot revisit this question block later. Below, the questions are listed with their answer choices and intended purposes:


Number|Question | Answer Choices | Intended purpose
-----------|-------------|----------------------|------------------------
1|What portion of your friends own pets?| a lot less than half, around half, a lot more than half | Does the turker live in a dog owning culture?
2| Please rank your preferences to work with the following media. | audio,text,images,other | Does the turker have a strong preference for image classification?
3| Have you ever lived with any dogs in your household? If not, have you ever planned to own a dog? | Yes, Maybe, No | Foes the turker pay attention to dog breeds at all?
4| On average, how many tasks on Amazon Mechanical Turk do you complete every week? | 0 to 10, 11 to 20, 21 to 30, 31 to 40, 40 or more | How much does the turker depend on Amt as a source of income?
5| Do you use Linkedin? (no need to provide any links) | Yes, No, Never heard of Linkedin | Does the turker has college or higher education? Does the turker take career development seriously?


Then, an external web link for dog breed references is provided, followed by 48 classification questions in multiple-choice format on the Qualtrics form. For the design of these classification questions, we chose eight dog breeds with a balance in size and hairy density (footnote).  Even numbered questions are harder and odd numbered questions are easier. A pilot was used to identify and filter out questions which all turkers scored correctly or incorrectly. The order of questions is randomized and show a balance of even and odd numbered questions even when we split the question set into three batches. Screener questions of cat images are mixed-in to help us identify those turkers who were not paying much attention to the task. All images come from the Stanford Dogs Dataset (footnote).


However, this simple mechanism poses a threat on the unbiasedness of our estimate. Since turkers self-select into HITs, HITs of different pay rates tend to attract different kinds of turkers. From our prior internet research, turkers tend to be strategic with how their time and expectation matches with pay rate, allotted time and nature of the posted HITs. If we randomize treatment pay rate at the posting level, we would be comparing groups of turkers with different attributes. Therefore, we came up with two experiment designs which branches from the basic mechanism described above. 


Design 1 is a traditional between-subject design, we define its unit of analysis as a HIT. Meaning, we place ourselves in the perspective of a data scientist in private industry who invest a company's money on getting human labeled examples for machine learning purposes. Our primary goal is to estimate how much more the company should spend on the AMT platform in order to get more accurate labeled training examples. With this motivation, we do not care about comparability of turker attributes, rather the returned accuracy per HIT as a result of different company spending. As such, selection bias and attrition from turkers are not concerns. (How do we randomize?)


Design 2 is a stepped-wedge design, we define its unit of analysis as a turker. Meaning, we place ourselves in the perspective of an economist, who studies the effect of incentives on labor productivity. Our primary goal is to estimate how increments of payment motivates a turker to perform better. With this motivation, unlike design 1, we care about comparability of turker attributes and want to ensure that treatment groups on average comprise of turkers of similar motivations and backgrounds. As such, selection bias and attrition are large concerns. In the following paragraphs we walk through each design in terms of level of randomization, treatments and execution protocol. 


In design 1, we randomize at the level of HIT postings. Over two weekends in November 2017, we released eight HIT postings, that is two for each of the four different pay rates. It is a traditional between subject design with clustered randomization. Since it would not be possible randomly post HITs one at a time, we posted them in batches of 100 HITs, each batch correspond to a single pay rate. We manually shuffle the order of postings to minimize order and time of day effects. The four pay rates are chosen according to the typical minimum and maximum of other HITs alike. Time frame for the eight postings do not overlap with each other. Design 1 details are summarized below:


\bigskip
\centerline{\Large Experiment Schedule for Design 1}


 Order | Date | Time Frame | Treatment (Pay Rate) | Available HITs
----------|-----------|-------------|----------------------
Pilot | Oct 28, 2017 (Saturday) | Morning | $0.10 | 50
Pilot | Oct 29, 2017 (Sunday) | Afternoon | $0.25 | 50
1 | Nov 11, 2017 (Saturday) | Morning | $0.10 | 100
1 | Nov 11, 2017 (Saturday) | Afternoon | $0.55 | 100
1 | Nov 12, 2017 (Sunday) | Morning | $0.25 | 100
1 | Nov 12, 2017 (Sunday) | Afternoon | $0.40 | 100
2 | Nov 18, 2017 (Saturday) | Morning | $0.40 | 100
2 | Nov 18, 2017 (Saturday) | Afternoon | $0.25 | 100
2 | Nov 19, 2017 (Sunday) | Morning | $0.55 | 100
2 | Nov 19, 2017 (Sunday) | Afternoon | $0.10 | 100

\bigskip
\centerline{\Large Design 1 Notation: Between Subject Design}
\bigskip
\centerline{\bfseries R T(0.10) O}
\centerline{\bfseries }
\centerline{\bfseries R T(0.25) O}
\centerline{\bfseries }
\centerline{\bfseries R T(0.40) O}
\centerline{\bfseries }
\centerline{\bfseries R T(0.55) O}
\bigskip



In design 2, we randomize at the level of turkers instead of postings. On November 26 2017 (Sunday), we released one HIT posting of 240 available HITs and baseline rate of \$0.22. It is a typical stepped-wedge design with randomization at the turkers level. Turkers would sign up for the HIT for the same baseline rate, and then randomized with equal probability into one of four treatment groups after they submitted their identification number and aptitude question answers on the Qualtrics survey form. The treatment group differs by the amount of surprise bonuses (up until this point the turker has no knowledge that this task may come with any bonuses). Here, the 48 dog breed classification questions from design 1 are split into three sesssions of 16 questions. The overall question order is the same as that in design 1, and the three sessions share a balance of difficulty and dog breeds. Each session is associated with a bonus assignment condition of either \$0.10 or nothing with no mention of bonus condition at all. We chose the baserate as \$0.22 rather than \$0.10 to minimize attrition and set the total available HITs to be 240 so to stay within experiment budget. Design 1 details are summarized below: 


\bigskip
\centerline{\Large Experiment Schedule for Design 2}


Name | Date | Time Frame | Base pay rate | Treatments (bonuses) | Available HITs
-----|----------|-----------|--------------|----------------------
Pilot |Nov 23, 2017 (Thursday) | All day | \$0.10 | \$0.00, \$0.05, \$0.10, \$0.15 | 60
Main  |Nov 26, 2017 (Sunday) | All day | \$0.22 | \$0.00, \$0.10, \$0.20, \$0.30 | 240


\bigskip
\centerline{\Large Design 2 Notation: Stepped-Wedge Design}
\bigskip
\centerline{\bfseries R C(0.00) O C(0.00) O C(0.00) O}
\centerline{\bfseries }
\centerline{\bfseries R C(0.00) O C(0.00) O T(0.10) O}
\centerline{\bfseries }
\centerline{\bfseries R C(0.00) O T(0.10) O T(0.10) O}
\centerline{\bfseries }
\centerline{\bfseries R T(0.10) O T(0.10) O T(0.10) O}
\bigskip


For both experiments, we took specific cautions in our execution protocol. Our pilots results indicated that although the pool size of Amazon turkers is in the order of hundred thousands, several turkers managed to find and submit our HIT for again but for a different pay rate. Additionally, some turkers may check out the HIT, go through the covariate questions, take a look at the dog breed classification questions, leave the HIT at one pay rate and sign up again for another higher pay rate. Therefore, in both designs, we assign turkers with "qualifications" -- labels with which we filter out turkers who have completed our HITs from the pool of turkers who may continue to see our following postings. We also keep a continuously updated list of identification numbers of those turkers who attrited, so to conditionally block them from accessing our Qualtrics survey. Because multiple attempts or preview of the same task under different treatment conditions would have carry-over or spill-over effects on the outcome, we felt that these cautions were necessary. On the other hand, differential attrition of turkers, although not specifically analyzed in design 1 (since our unit of analysis is defined as the returned HIT rather than the turker), was conspicuous in the data. To mitigate the problem that turkers who started in lower pay rate postings tend to attrite more than those who started in higher pay rate postings, we raise the base rate in design 2, in which turkers are our unit of analysis, from \$0.10 to \$0.22. The results section will give describe attrition data in detail.


***

A note on supplementary files: A clean, well organized Github repo is available for this project. It includes raw, intermediate and transformed data, along with data transformation, statistical and project management codes. Because of the length of extensiveness of these R functions, we decided to print such functions as a higher level of abstraction and maintain a natural presentation flow of the project. For project evaluation's sake, please refer to the .rmd file for our the hidden snippets, and our Github repo for any other materials. (Available upon request)

***

# Design 1 Results

## Pilot Study (Design 1)

By running a pilot study for design 1, we tested the experiment protocol, identified problems in our AMT and Qualtrics workflow and conducted a power analysis on the collected data. During the last weekend of Oct 2017, we published two non-overlapping HIT postings each with 50 HITs available. One at the treatment pay rate of \$0.10 on Saturday and the other at the treatment pay rate of \$0.25 on Sunday. We tried to minimize differences in launching conditions for the two postings so to ensure comparability. 

```{r, include=FALSE}
# read in Qualtrics output csv
qualtric_pilot_data_path_0.25 = "../qualtric_data/20171028_qualtric_results_pilot_0.25.csv"
current_task_pilot_data_0.25 = get_current_task_data(qualtric_pilot_data_path_0.25)
qualtric_pilot_data_path_0.10 = "../qualtric_data/20171028_qualtric_results_pilot_0.10.csv"
current_task_pilot_data_0.10 = get_current_task_data(qualtric_pilot_data_path_0.10)

# evaluate accuracies and attach covariates per HIT level
worker_perf_pilot_0.25 = evaluate_worker_perf(current_task_pilot_data_0.25, allQ)
worker_perf_pilot_0.10 = evaluate_worker_perf(current_task_pilot_data_0.10, allQ)

# pool the data from different treatments together
worker_perf_pilot_0.25$treatment = 0.25
worker_perf_pilot_0.10$treatment = 0.10
regr_table_pilot = rbind(worker_perf_pilot_0.10, worker_perf_pilot_0.25)

# transform pre-treatment covariates CQ1, CQ2_3, CQ3
regr_table_pilot$CQ1 = as.factor(regr_table_pilot$CQ1)
regr_table_pilot$CQ2_3 = as.numeric(regr_table_pilot$CQ2_3)
regr_table_pilot$CQ3 = as.factor(regr_table_pilot$CQ3)
```

### Data (Design 1 Pilot)

The below table shows a summary of the two pilot postings and corresponding average accuracies. In comparison, we can see that the posting that paid more returned a higher average accuracy and completed faster than the lower paying posting. Note that the number of returned HITs in each posting is higher than 50 because some turkers may submit the HIT before the Qualtrics survey terminates but after the AMT posting terminates.

```{r, echo=FALSE}
smry_025 = summarize_worker_perf(current_task_pilot_data_0.25, allQ)
smry_010 = summarize_worker_perf(current_task_pilot_data_0.10, allQ)

avg_time_025 = mean(as.numeric(regr_table_pilot[regr_table_pilot$treatment == 0.25,]$time_spent))/60
avg_time_010= mean(as.numeric(regr_table_pilot[regr_table_pilot$treatment == 0.10,]$time_spent))/60

pilot_summary = data.frame(Name=c("Pilot 1", "Pilot 2"), Treatment=c("$0.10","$0.25"),
                           N=c(nrow(worker_perf_pilot_0.10),nrow(worker_perf_pilot_0.25)),
                           TotalTime=c("2h 30min","1h 20min"), 
                           AvgTimePerTask = c(paste(toString(round(avg_time_010,3)),"min",sep=""),
                                              paste(toString(round(avg_time_025,3)),"min",sep="")
                                              ), 
                           AccuracyMean=c(round(smry_010$mean,3),round(smry_025$mean,3)), 
                           AccuracySd=c(round(smry_010$std,3),round(smry_025$std,3)))
knitr::kable(pilot_summary, "markdown")
```

The overall accuracy distribution is bimodal. One mode occurs between [0.15,0.20] and the other occurs between [0.85,0.95]. The distribution of HITs listed for \$0.10 bias towards the first mode, while that for \$0.25 bias towards the second mode. This is inline with our expectation that turkers are either accomplish with determination or care little (given eight choices for each question, making random choices would yield an accuracy of 0.125 in expectation). The fact that this accuracy distribution is non-normal cautioned us against reliance on OSL asymptotics for standard error estimation. While a larger sample size can increase this reliability, we nevertheless plan to include randomization inference on top of the t-statistic from OLS.  

```{r, echo=FALSE}
# overall accuracy plot
hist.design1pilot = ggplot(regr_table_pilot, aes(accuracy, fill=factor(treatment))) +
  geom_histogram(binwidth = .1) +
  labs(x="Accuracy in Proportion", y = "count", fill="accuracy") +
  ggtitle(label="HIT Accuracies in Design 1 Pilot") +
  theme_gray()
hist.design1pilot 
```

### Covariate Balance (Design 1 Pilot)

Of the 5 aptitude questions we asked of our turkers, we believe that responses to question 1, 2 and 3 do not depend on the treatment assignment, since turkers have no knowledge of the task being related to image classification nor dog breeds until this point of the survey. In contrast, responses to question 4 and 5 probes the turkers' income and education level, so they are prone to selection bias associated with the posted HIT payrate. Therefore, we assume responses to the question 1, 2 and 3 are useful controls while the other two are bad controls. To conduct a covariate balance check, we regress responses to question 1, 2 and 3 on the treatment variable. The regression table summarizes that treatment fails to predict any of the answers in a statistically significant way. Our covariate balance check has passed.


```{r, include=F}
# Dog friends question
CQ1_1 = regr_table_pilot$CQ1 == "a lot less than half"
CQ1_2 = regr_table_pilot$CQ1 == "around half"
CQ1_3 = regr_table_pilot$CQ1 == "a lot more than half"
cov_regr_CQ1_1= lm(CQ1_1 ~ regr_table_pilot$treatment)
cov_regr_CQ1_2= lm(CQ1_2 ~ regr_table_pilot$treatment)
cov_regr_CQ1_3= lm(CQ1_3 ~ regr_table_pilot$treatment)

# Preference to work with images question
cov_regr_CQ2_3= lm(CQ2_3 ~ regr_table_pilot$treatment, data = regr_table_pilot)

# Lived with or planned to own a dog
CQ3_1 = regr_table_pilot$CQ3 == "Yes"
CQ3_2 = regr_table_pilot$CQ3 == "No"
CQ3_3 = regr_table_pilot$CQ3 == "Maybe"
cov_regr_CQ3_1= lm(CQ3_1 ~ regr_table_pilot$treatment)
cov_regr_CQ3_2= lm(CQ3_2 ~ regr_table_pilot$treatment)
cov_regr_CQ3_3= lm(CQ3_3 ~ regr_table_pilot$treatment)

(se.cov_regr_CQ1_1 = lmtest::coeftest(cov_regr_CQ1_1, vcov = vcovHC)[ , "Std. Error"])
(se.cov_regr_CQ1_2 = lmtest::coeftest(cov_regr_CQ1_2, vcov = vcovHC)[ , "Std. Error"])
(se.cov_regr_CQ1_3 = lmtest::coeftest(cov_regr_CQ1_3, vcov = vcovHC)[ , "Std. Error"])
(se.cov_regr_CQ2_3 = lmtest::coeftest(cov_regr_CQ2_3, vcov = vcovHC)[ , "Std. Error"])
(se.cov_regr_CQ3_1 = lmtest::coeftest(cov_regr_CQ3_1, vcov = vcovHC)[ , "Std. Error"])
(se.cov_regr_CQ3_2 = lmtest::coeftest(cov_regr_CQ3_1, vcov = vcovHC)[ , "Std. Error"])
(se.cov_regr_CQ3_3 = lmtest::coeftest(cov_regr_CQ3_1, vcov = vcovHC)[ , "Std. Error"])
```

```{r, echo=F, warning=F}
stargazer::stargazer(cov_regr_CQ1_1, cov_regr_CQ1_2, cov_regr_CQ1_3,
                     cov_regr_CQ2_3,
                     cov_regr_CQ3_1, cov_regr_CQ3_2, cov_regr_CQ3_3,
                     type = "text", title = "Covariate Balance Check", style = "default",
                     se = list(se.cov_regr_CQ1_1, se.cov_regr_CQ1_2, se.cov_regr_CQ1_3,
                               se.cov_regr_CQ2_3,
                               se.cov_regr_CQ3_1, se.cov_regr_CQ3_2, se.cov_regr_CQ3_3),
                     star.cutoffs = c(0.05, 0.01, 0.001),
                     model.names = F
                     )
```

> CQ1_1 indicates if the turker has a lot less than half of friends who own pets, CQ1_2 indicates if the turker has around half of friends who own pets, CQ1_3 indicates if the turker has a lot more than half of friends who own pets, CQ2_3 ranks turkers' preference to work with images, CQ3_1 indicates that the turker has lived with or planned to own a dog, CQ3_2 indicates that the turker may have planned to own a dog, CQ3_3 indicates that the turker has never lived with or planned to own a dog

### Treatment Effect Estimation (Design 1 Pilot)

We performed a basic power analysis on our pilot data, so we could get an initial feel of the results we would be getting. First, we conducted a Levene test, which showed us that the variances of the \$0.10 outcomes and the \$0.25 outcomes are similar. From there, we ran a two-sample independent t-test, as well as a simple and a full regression with robust standard error. 

The two-sample independent t-test results, which is equivalent to OLS results with only one variable, shows marginal significance for the treatment effect. The full-regression shows no statistical significance (p-val ~ 0.07) for the treatment effect. These results lead us to believe that there might be statistical significance in the main experiment we planned. Therefore, for in the main experiment for design 1, we decided to raise sample size for each posting from 50 to 100, and further expand our treatments to include \$0.40 and \$0.55.

```{r, echo=F, warning=F}
car::leveneTest(worker_perf_pilot_0.10$accuracy,worker_perf_pilot_0.25$accuracy,center=median)
```

```{r, echo=F, warning=F}
t.test(worker_perf_pilot_0.10$accuracy,
       worker_perf_pilot_0.25$accuracy,
       alternative = "two.sided", var.equal = TRUE)
```

```{r, include=F}
regr1_simple_pilot = lm(accuracy ~ treatment, data = regr_table_pilot)
regr2_simple_pilot = lm(accuracy ~ treatment + CQ1 + CQ2_3 + CQ3, data = regr_table_pilot)
(se.regr1_simple_pilot = lmtest::coeftest(regr1_simple_pilot, vcov = vcovHC)[ , "Std. Error"])
(se.regr2_simple_pilot = lmtest::coeftest(regr2_simple_pilot, vcov = vcovHC)[ , "Std. Error"])
```

```{r, echo=F, warning=F}
stargazer::stargazer(regr1_simple_pilot, regr2_simple_pilot,
                     type = "text", title = "Covariate Balance Check", style = "default",
                     se = list(se.regr1_simple_pilot, se.regr2_simple_pilot),
                     star.cutoffs = c(0.05, 0.01, 0.001),
                     column.labels = c("simple","full"),
                     model.names = F
                     )
```

(Write code here for robust p values and CI)

## Main experiment (Design 1)

By running a pilot study for design 1, we tested the experiment protocol, identified problems in our AMT and Qualtrics workflow and conducted a power analysis on the collected data. During the last weekend of Oct 2017, we published two non-overlapping HIT postings each with 50 HITs available. One at the treatment pay rate of \$0.10 on Saturday and the other at the treatment pay rate of \$0.25 on Sunday. We tried to minimize differences in launching conditions for the two postings so to ensure comparability. 

### Data (Design 1 Main)

### Covariate Balance (Design 1 Main)

### Treatment Effect Estimation (Design 1 Main)




# Design 2 Results

## Pilot Study (Design 2)

## Main experiment (Design 2)

### Data (Design 2 Main)

### Covariate Balance (Design 2 Main)

### Treatment Effect Estimation (Design 2 Main)



# Future Lines of investigation

# Conclusion and Bibliography